#!/usr/bin/env python3
"""
PostgreSQLä»»åŠ¡é˜Ÿåˆ—æ€§èƒ½æµ‹è¯•
è¯¦ç»†æµ‹è¯•å„é¡¹æ€§èƒ½æŒ‡æ ‡å’Œç“¶é¢ˆåˆ†æ
"""

import asyncio
import sys
import time
import statistics
from pathlib import Path
from datetime import datetime, timezone
import concurrent.futures
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app.services.postgresql_task_processor import task_queue
from app.utils.logger import get_logger

logger = get_logger(__name__)


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.enqueue_times = []
        self.query_times = []
        self.stats_times = []
        self.total_operations = 0
        self.errors = []
        self.start_time = None
        self.end_time = None
    
    def add_enqueue_time(self, duration: float):
        self.enqueue_times.append(duration)
        self.total_operations += 1
    
    def add_query_time(self, duration: float):
        self.query_times.append(duration)
        self.total_operations += 1
    
    def add_stats_time(self, duration: float):
        self.stats_times.append(duration)
        self.total_operations += 1
    
    def add_error(self, error: str):
        self.errors.append(error)
    
    def start_timer(self):
        self.start_time = time.time()
    
    def stop_timer(self):
        self.end_time = time.time()
    
    @property
    def total_duration(self) -> float:
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        def calc_stats(times: List[float]) -> Dict[str, float]:
            if not times:
                return {"count": 0, "avg": 0, "min": 0, "max": 0, "p95": 0, "p99": 0}
            
            return {
                "count": len(times),
                "avg": statistics.mean(times),
                "min": min(times),
                "max": max(times),
                "p95": statistics.quantiles(times, n=20)[18] if len(times) > 20 else max(times),
                "p99": statistics.quantiles(times, n=100)[98] if len(times) > 100 else max(times)
            }
        
        return {
            "total_duration": self.total_duration,
            "total_operations": self.total_operations,
            "operations_per_second": self.total_operations / self.total_duration if self.total_duration > 0 else 0,
            "error_count": len(self.errors),
            "error_rate": len(self.errors) / self.total_operations if self.total_operations > 0 else 0,
            "enqueue_performance": calc_stats(self.enqueue_times),
            "query_performance": calc_stats(self.query_times),
            "stats_performance": calc_stats(self.stats_times),
            "errors": self.errors[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
        }


async def test_single_operation_latency():
    """æµ‹è¯•å•æ“ä½œå»¶è¿Ÿ"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•1: å•æ“ä½œå»¶è¿Ÿåˆ†æ")
    logger.info("=" * 60)
    
    metrics = PerformanceMetrics()
    metrics.start_timer()
    
    try:
        # æµ‹è¯•ä»»åŠ¡å…¥é˜Ÿå»¶è¿Ÿ
        logger.info("ğŸ”„ æµ‹è¯•ä»»åŠ¡å…¥é˜Ÿå»¶è¿Ÿ...")
        
        for i in range(10):
            start_time = time.time()
            
            task_id = await task_queue.enqueue(
                task_type="ocr_extract",
                payload={"test": f"latency_test_{i}", "index": i},
                priority=5,
                correlation_id=f"latency-test-{i}"
            )
            
            end_time = time.time()
            metrics.add_enqueue_time(end_time - start_time)
            
            logger.info(f"   ä»»åŠ¡ {i+1}: {(end_time - start_time)*1000:.2f}ms")
        
        # æµ‹è¯•çŠ¶æ€æŸ¥è¯¢å»¶è¿Ÿ
        logger.info("ğŸ” æµ‹è¯•çŠ¶æ€æŸ¥è¯¢å»¶è¿Ÿ...")
        
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•ä»»åŠ¡ç”¨äºæŸ¥è¯¢
        test_task_id = await task_queue.enqueue(
            task_type="send_notification",
            payload={"test": "query_test"},
            correlation_id="query-test"
        )
        
        for i in range(10):
            start_time = time.time()
            
            status = await task_queue.get_task_status(test_task_id)
            
            end_time = time.time()
            metrics.add_query_time(end_time - start_time)
            
            logger.info(f"   æŸ¥è¯¢ {i+1}: {(end_time - start_time)*1000:.2f}ms")
        
        # æµ‹è¯•ç»Ÿè®¡æŸ¥è¯¢å»¶è¿Ÿ
        logger.info("ğŸ“Š æµ‹è¯•ç»Ÿè®¡æŸ¥è¯¢å»¶è¿Ÿ...")
        
        for i in range(5):
            start_time = time.time()
            
            stats = await task_queue.get_task_stats(hours_back=1)
            
            end_time = time.time()
            metrics.add_stats_time(end_time - start_time)
            
            logger.info(f"   ç»Ÿè®¡ {i+1}: {(end_time - start_time)*1000:.2f}ms ({len(stats)} æ¡è®°å½•)")
        
        metrics.stop_timer()
        return metrics.get_summary()
        
    except Exception as e:
        logger.error(f"å•æ“ä½œå»¶è¿Ÿæµ‹è¯•å¤±è´¥: {e}")
        metrics.add_error(str(e))
        metrics.stop_timer()
        return metrics.get_summary()


async def test_batch_operations():
    """æµ‹è¯•æ‰¹é‡æ“ä½œæ€§èƒ½"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•2: æ‰¹é‡æ“ä½œæ€§èƒ½")
    logger.info("=" * 60)
    
    batch_sizes = [10, 50, 100]
    results = {}
    
    for batch_size in batch_sizes:
        logger.info(f"ğŸš€ æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
        
        metrics = PerformanceMetrics()
        metrics.start_timer()
        
        try:
            # æ‰¹é‡å…¥é˜Ÿ
            start_time = time.time()
            
            tasks = []
            for i in range(batch_size):
                task_coro = task_queue.enqueue(
                    task_type="cleanup_files",
                    payload={
                        "test": f"batch_test_{batch_size}_{i}",
                        "batch_size": batch_size,
                        "index": i
                    },
                    priority=3,
                    correlation_id=f"batch-{batch_size}-{i}"
                )
                tasks.append(task_coro)
            
            # å¹¶å‘æ‰§è¡Œ
            task_ids = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            batch_duration = end_time - start_time
            
            # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
            successful_tasks = [tid for tid in task_ids if isinstance(tid, str)]
            failed_tasks = [tid for tid in task_ids if isinstance(tid, Exception)]
            
            logger.info(f"   æ‰¹é‡å…¥é˜Ÿå®Œæˆ:")
            logger.info(f"     æˆåŠŸ: {len(successful_tasks)} ä¸ª")
            logger.info(f"     å¤±è´¥: {len(failed_tasks)} ä¸ª")
            logger.info(f"     æ€»è€—æ—¶: {batch_duration:.3f}s")
            logger.info(f"     ååé‡: {len(successful_tasks)/batch_duration:.1f} ä»»åŠ¡/ç§’")
            
            # æ‰¹é‡æŸ¥è¯¢çŠ¶æ€
            if successful_tasks:
                query_start = time.time()
                
                status_tasks = []
                for task_id in successful_tasks[:10]:  # åªæŸ¥è¯¢å‰10ä¸ª
                    status_coro = task_queue.get_task_status(task_id)
                    status_tasks.append(status_coro)
                
                statuses = await asyncio.gather(*status_tasks, return_exceptions=True)
                
                query_end = time.time()
                query_duration = query_end - query_start
                
                successful_queries = [s for s in statuses if isinstance(s, dict)]
                
                logger.info(f"   æ‰¹é‡æŸ¥è¯¢å®Œæˆ:")
                logger.info(f"     æŸ¥è¯¢æ•°é‡: {len(status_tasks)} ä¸ª")
                logger.info(f"     æˆåŠŸ: {len(successful_queries)} ä¸ª")
                logger.info(f"     è€—æ—¶: {query_duration:.3f}s")
                logger.info(f"     æŸ¥è¯¢ååé‡: {len(successful_queries)/query_duration:.1f} æŸ¥è¯¢/ç§’")
            
            metrics.stop_timer()
            
            results[batch_size] = {
                "batch_size": batch_size,
                "successful_enqueue": len(successful_tasks),
                "failed_enqueue": len(failed_tasks),
                "enqueue_duration": batch_duration,
                "enqueue_throughput": len(successful_tasks) / batch_duration,
                "query_duration": query_duration if successful_tasks else 0,
                "query_throughput": len(successful_queries) / query_duration if successful_tasks and query_duration > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ“ä½œæµ‹è¯•å¤±è´¥ (å¤§å° {batch_size}): {e}")
            results[batch_size] = {"error": str(e)}
    
    return results


async def test_concurrent_access():
    """æµ‹è¯•å¹¶å‘è®¿é—®æ€§èƒ½"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•3: å¹¶å‘è®¿é—®æ€§èƒ½")
    logger.info("=" * 60)
    
    concurrent_levels = [5, 10, 20]
    results = {}
    
    for concurrency in concurrent_levels:
        logger.info(f"ğŸ”€ æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
        
        try:
            async def worker_task(worker_id: int):
                """å·¥ä½œå™¨ä»»åŠ¡"""
                worker_metrics = PerformanceMetrics()
                worker_metrics.start_timer()
                
                operations = 10  # æ¯ä¸ªå·¥ä½œå™¨æ‰§è¡Œ10ä¸ªæ“ä½œ
                
                for i in range(operations):
                    try:
                        # å…¥é˜Ÿæ“ä½œ
                        start_time = time.time()
                        task_id = await task_queue.enqueue(
                            task_type="process_email",
                            payload={
                                "test": f"concurrent_test_{concurrency}_{worker_id}_{i}",
                                "worker_id": worker_id,
                                "operation": i
                            },
                            priority=1,
                            correlation_id=f"concurrent-{concurrency}-{worker_id}-{i}"
                        )
                        end_time = time.time()
                        worker_metrics.add_enqueue_time(end_time - start_time)
                        
                        # æŸ¥è¯¢æ“ä½œ
                        start_time = time.time()
                        status = await task_queue.get_task_status(task_id)
                        end_time = time.time()
                        worker_metrics.add_query_time(end_time - start_time)
                        
                    except Exception as e:
                        worker_metrics.add_error(f"Worker {worker_id}, Op {i}: {str(e)}")
                
                worker_metrics.stop_timer()
                return worker_metrics.get_summary()
            
            # å¯åŠ¨å¹¶å‘å·¥ä½œå™¨
            start_time = time.time()
            
            workers = []
            for worker_id in range(concurrency):
                workers.append(worker_task(worker_id))
            
            worker_results = await asyncio.gather(*workers, return_exceptions=True)
            
            end_time = time.time()
            total_duration = end_time - start_time
            
            # æ±‡æ€»ç»“æœ
            successful_workers = [r for r in worker_results if isinstance(r, dict)]
            failed_workers = [r for r in worker_results if isinstance(r, Exception)]
            
            total_operations = sum(r["total_operations"] for r in successful_workers)
            total_errors = sum(r["error_count"] for r in successful_workers)
            
            avg_enqueue_time = statistics.mean([
                r["enqueue_performance"]["avg"] 
                for r in successful_workers 
                if r["enqueue_performance"]["count"] > 0
            ]) if successful_workers else 0
            
            avg_query_time = statistics.mean([
                r["query_performance"]["avg"] 
                for r in successful_workers 
                if r["query_performance"]["count"] > 0
            ]) if successful_workers else 0
            
            logger.info(f"   å¹¶å‘æµ‹è¯•å®Œæˆ:")
            logger.info(f"     å¹¶å‘å·¥ä½œå™¨: {len(successful_workers)}/{concurrency}")
            logger.info(f"     æ€»æ“ä½œæ•°: {total_operations}")
            logger.info(f"     æ€»é”™è¯¯æ•°: {total_errors}")
            logger.info(f"     æ€»è€—æ—¶: {total_duration:.3f}s")
            logger.info(f"     æ•´ä½“ååé‡: {total_operations/total_duration:.1f} æ“ä½œ/ç§’")
            logger.info(f"     å¹³å‡å…¥é˜Ÿå»¶è¿Ÿ: {avg_enqueue_time*1000:.2f}ms")
            logger.info(f"     å¹³å‡æŸ¥è¯¢å»¶è¿Ÿ: {avg_query_time*1000:.2f}ms")
            
            results[concurrency] = {
                "concurrency": concurrency,
                "successful_workers": len(successful_workers),
                "failed_workers": len(failed_workers),
                "total_operations": total_operations,
                "total_errors": total_errors,
                "total_duration": total_duration,
                "overall_throughput": total_operations / total_duration,
                "avg_enqueue_latency_ms": avg_enqueue_time * 1000,
                "avg_query_latency_ms": avg_query_time * 1000,
                "error_rate": total_errors / total_operations if total_operations > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"å¹¶å‘æµ‹è¯•å¤±è´¥ (å¹¶å‘çº§åˆ« {concurrency}): {e}")
            results[concurrency] = {"error": str(e)}
    
    return results


async def test_database_performance():
    """æµ‹è¯•æ•°æ®åº“å±‚é¢æ€§èƒ½"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•4: æ•°æ®åº“å±‚é¢æ€§èƒ½")
    logger.info("=" * 60)
    
    import asyncpg
    from app.core.config import settings
    
    try:
        # ç›´æ¥æ•°æ®åº“è¿æ¥æµ‹è¯•
        db_url = settings.database_url.replace('postgresql+asyncpg://', 'postgresql://').replace('postgresql+psycopg://', 'postgresql://')
        
        # è¿æ¥æ—¶é—´æµ‹è¯•
        logger.info("ğŸ”— æµ‹è¯•æ•°æ®åº“è¿æ¥æ€§èƒ½...")
        
        connection_times = []
        for i in range(5):
            start_time = time.time()
            conn = await asyncpg.connect(db_url)
            end_time = time.time()
            
            connection_time = end_time - start_time
            connection_times.append(connection_time)
            
            await conn.close()
            
            logger.info(f"   è¿æ¥ {i+1}: {connection_time*1000:.2f}ms")
        
        avg_connection_time = statistics.mean(connection_times)
        logger.info(f"   å¹³å‡è¿æ¥æ—¶é—´: {avg_connection_time*1000:.2f}ms")
        
        # æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
        logger.info("ğŸ“Š æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½...")
        
        conn = await asyncpg.connect(db_url)
        
        try:
            # ç®€å•æŸ¥è¯¢
            start_time = time.time()
            result = await conn.fetchval("SELECT COUNT(*) FROM task_queue")
            end_time = time.time()
            simple_query_time = end_time - start_time
            
            logger.info(f"   ç®€å•è®¡æ•°æŸ¥è¯¢: {simple_query_time*1000:.2f}ms (ç»“æœ: {result})")
            
            # å¤æ‚æŸ¥è¯¢
            start_time = time.time()
            results = await conn.fetch("""
                SELECT task_type, status, COUNT(*) as count
                FROM task_queue 
                WHERE created_at >= NOW() - INTERVAL '1 hour'
                GROUP BY task_type, status
                ORDER BY task_type, status
            """)
            end_time = time.time()
            complex_query_time = end_time - start_time
            
            logger.info(f"   å¤æ‚ç»Ÿè®¡æŸ¥è¯¢: {complex_query_time*1000:.2f}ms (ç»“æœ: {len(results)} è¡Œ)")
            
            # å‡½æ•°è°ƒç”¨æ€§èƒ½
            start_time = time.time()
            task_id = await conn.fetchval("""
                SELECT enqueue_task(
                    'ocr_extract'::task_type,
                    '{"test": "db_performance"}'::jsonb,
                    NULL,
                    5,
                    0,
                    3,
                    'db-perf-test'
                )
            """)
            end_time = time.time()
            function_call_time = end_time - start_time
            
            logger.info(f"   å‡½æ•°è°ƒç”¨ (å…¥é˜Ÿ): {function_call_time*1000:.2f}ms")
            
            # çŠ¶æ€æŸ¥è¯¢
            start_time = time.time()
            status_result = await conn.fetchrow("""
                SELECT id, task_type, status, created_at
                FROM task_queue 
                WHERE id = $1
            """, task_id)
            end_time = time.time()
            status_query_time = end_time - start_time
            
            logger.info(f"   çŠ¶æ€æŸ¥è¯¢: {status_query_time*1000:.2f}ms")
            
        finally:
            await conn.close()
        
        return {
            "avg_connection_time_ms": avg_connection_time * 1000,
            "simple_query_time_ms": simple_query_time * 1000,
            "complex_query_time_ms": complex_query_time * 1000,
            "function_call_time_ms": function_call_time * 1000,
            "status_query_time_ms": status_query_time * 1000,
            "total_tasks_in_db": result
        }
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}


async def generate_performance_report():
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    logger.info("ğŸš€ å¼€å§‹PostgreSQLä»»åŠ¡é˜Ÿåˆ—æ€§èƒ½æµ‹è¯•")
    logger.info(f"æµ‹è¯•æ—¶é—´: {datetime.now(timezone.utc).isoformat()}")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("å•æ“ä½œå»¶è¿Ÿ", test_single_operation_latency),
        ("æ‰¹é‡æ“ä½œæ€§èƒ½", test_batch_operations),
        ("å¹¶å‘è®¿é—®æ€§èƒ½", test_concurrent_access),
        ("æ•°æ®åº“å±‚é¢æ€§èƒ½", test_database_performance),
    ]
    
    results = {}
    total_start_time = time.time()
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*20} å¼€å§‹ {test_name} {'='*20}")
        
        try:
            test_start = time.time()
            result = await test_func()
            test_end = time.time()
            
            results[test_name] = {
                "result": result,
                "test_duration": test_end - test_start,
                "status": "success"
            }
            
            logger.info(f"âœ… {test_name} å®Œæˆ (è€—æ—¶: {test_end - test_start:.2f}s)")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} å¤±è´¥: {e}")
            results[test_name] = {
                "error": str(e),
                "status": "failed"
            }
    
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    
    # ç”ŸæˆæŠ¥å‘Š
    logger.info("\n" + "=" * 80)
    logger.info("PostgreSQLä»»åŠ¡é˜Ÿåˆ—æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    logger.info("=" * 80)
    
    logger.info(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
    logger.info(f"â±ï¸  æ€»æµ‹è¯•æ—¶é•¿: {total_duration:.2f}ç§’")
    logger.info(f"âœ… æˆåŠŸæµ‹è¯•: {sum(1 for r in results.values() if r['status'] == 'success')}/{len(tests)}")
    
    # è¯¦ç»†ç»“æœåˆ†æ
    for test_name, test_result in results.items():
        logger.info(f"\nğŸ“Š {test_name} è¯¦ç»†ç»“æœ:")
        
        if test_result["status"] == "success":
            result_data = test_result["result"]
            
            if test_name == "å•æ“ä½œå»¶è¿Ÿ":
                enqueue_perf = result_data["enqueue_performance"]
                query_perf = result_data["query_performance"]
                stats_perf = result_data["stats_performance"]
                
                logger.info(f"   å…¥é˜Ÿæ“ä½œ: å¹³å‡ {enqueue_perf['avg']*1000:.2f}ms, P95 {enqueue_perf['p95']*1000:.2f}ms")
                logger.info(f"   æŸ¥è¯¢æ“ä½œ: å¹³å‡ {query_perf['avg']*1000:.2f}ms, P95 {query_perf['p95']*1000:.2f}ms")
                logger.info(f"   ç»Ÿè®¡æŸ¥è¯¢: å¹³å‡ {stats_perf['avg']*1000:.2f}ms")
                
            elif test_name == "æ‰¹é‡æ“ä½œæ€§èƒ½":
                for batch_size, data in result_data.items():
                    if isinstance(data, dict) and "error" not in data:
                        logger.info(f"   æ‰¹é‡å¤§å° {batch_size}: å…¥é˜Ÿååé‡ {data['enqueue_throughput']:.1f} ä»»åŠ¡/ç§’")
                        
            elif test_name == "å¹¶å‘è®¿é—®æ€§èƒ½":
                for concurrency, data in result_data.items():
                    if isinstance(data, dict) and "error" not in data:
                        logger.info(f"   å¹¶å‘çº§åˆ« {concurrency}: ååé‡ {data['overall_throughput']:.1f} æ“ä½œ/ç§’, é”™è¯¯ç‡ {data['error_rate']*100:.1f}%")
                        
            elif test_name == "æ•°æ®åº“å±‚é¢æ€§èƒ½":
                if "error" not in result_data:
                    logger.info(f"   è¿æ¥æ—¶é—´: {result_data['avg_connection_time_ms']:.2f}ms")
                    logger.info(f"   ç®€å•æŸ¥è¯¢: {result_data['simple_query_time_ms']:.2f}ms")
                    logger.info(f"   å‡½æ•°è°ƒç”¨: {result_data['function_call_time_ms']:.2f}ms")
        else:
            logger.error(f"   âŒ æµ‹è¯•å¤±è´¥: {test_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æ€§èƒ½æ€»ç»“
    logger.info(f"\nğŸ¯ æ€§èƒ½æ€»ç»“:")
    
    # æå–å…³é”®æŒ‡æ ‡
    if "å•æ“ä½œå»¶è¿Ÿ" in results and results["å•æ“ä½œå»¶è¿Ÿ"]["status"] == "success":
        single_op = results["å•æ“ä½œå»¶è¿Ÿ"]["result"]
        avg_enqueue = single_op["enqueue_performance"]["avg"] * 1000
        avg_query = single_op["query_performance"]["avg"] * 1000
        
        logger.info(f"   âš¡ å¹³å‡å…¥é˜Ÿå»¶è¿Ÿ: {avg_enqueue:.1f}ms")
        logger.info(f"   âš¡ å¹³å‡æŸ¥è¯¢å»¶è¿Ÿ: {avg_query:.1f}ms")
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        if avg_enqueue < 100 and avg_query < 50:
            logger.info("   ğŸš€ æ€§èƒ½ç­‰çº§: ä¼˜ç§€")
        elif avg_enqueue < 500 and avg_query < 200:
            logger.info("   âœ… æ€§èƒ½ç­‰çº§: è‰¯å¥½")
        elif avg_enqueue < 1000 and avg_query < 500:
            logger.info("   âš ï¸  æ€§èƒ½ç­‰çº§: ä¸€èˆ¬")
        else:
            logger.info("   âŒ æ€§èƒ½ç­‰çº§: éœ€è¦ä¼˜åŒ–")
    
    # å»ºè®®
    logger.info(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    
    if "æ•°æ®åº“å±‚é¢æ€§èƒ½" in results and results["æ•°æ®åº“å±‚é¢æ€§èƒ½"]["status"] == "success":
        db_perf = results["æ•°æ®åº“å±‚é¢æ€§èƒ½"]["result"]
        if "avg_connection_time_ms" in db_perf and db_perf["avg_connection_time_ms"] > 100:
            logger.info("   ğŸ”§ å»ºè®®: å®ç°è¿æ¥æ± ä»¥å‡å°‘è¿æ¥å¼€é”€")
    
    if "å¹¶å‘è®¿é—®æ€§èƒ½" in results and results["å¹¶å‘è®¿é—®æ€§èƒ½"]["status"] == "success":
        concurrent_perf = results["å¹¶å‘è®¿é—®æ€§èƒ½"]["result"]
        high_error_rates = [
            data.get("error_rate", 0) for data in concurrent_perf.values() 
            if isinstance(data, dict) and "error_rate" in data
        ]
        if any(rate > 0.1 for rate in high_error_rates):
            logger.info("   ğŸ”§ å»ºè®®: ä¼˜åŒ–é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
    
    logger.info("   ğŸ”§ å»ºè®®: è€ƒè™‘æ·»åŠ æœ¬åœ°ç¼“å­˜å‡å°‘æ•°æ®åº“æŸ¥è¯¢")
    logger.info("   ğŸ”§ å»ºè®®: å®ç°æ‰¹é‡æ“ä½œAPIæé«˜ååé‡")
    
    return results


async def main():
    """ä¸»å‡½æ•°"""
    try:
        results = await generate_performance_report()
        
        # è®¡ç®—æ€»ä½“æˆåŠŸç‡
        successful_tests = sum(1 for r in results.values() if r["status"] == "success")
        total_tests = len(results)
        success_rate = successful_tests / total_tests * 100
        
        logger.info(f"\nğŸ æµ‹è¯•å®Œæˆ! æˆåŠŸç‡: {success_rate:.1f}% ({successful_tests}/{total_tests})")
        
        return 0 if success_rate >= 75 else 1
        
    except Exception as e:
        logger.error(f"æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)