#!/usr/bin/env python3
"""
å¿«é€Ÿæ€§èƒ½è¯Šæ–­æµ‹è¯•
å¿«é€Ÿå®šä½PostgreSQLé˜Ÿåˆ—çš„æ€§èƒ½ç“¶é¢ˆ
"""

import asyncio
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

import asyncpg
from app.core.config import settings
from app.utils.logger import get_logger

logger = get_logger(__name__)


async def test_raw_database_connection():
    """æµ‹è¯•åŸå§‹æ•°æ®åº“è¿æ¥æ€§èƒ½"""
    logger.info("ğŸ”— æµ‹è¯•åŸå§‹æ•°æ®åº“è¿æ¥æ€§èƒ½")
    
    db_url = settings.database_url.replace('postgresql+asyncpg://', 'postgresql://').replace('postgresql+psycopg://', 'postgresql://')
    logger.info(f"è¿æ¥URL: {db_url.split('@')[1] if '@' in db_url else 'masked'}")
    
    # æµ‹è¯•è¿æ¥æ—¶é—´
    start_time = time.time()
    try:
        conn = await asyncpg.connect(db_url, statement_cache_size=0)
        connect_time = time.time() - start_time
        logger.info(f"âœ… è¿æ¥æˆåŠŸ: {connect_time*1000:.2f}ms")
        
        # æµ‹è¯•ç®€å•æŸ¥è¯¢
        start_time = time.time()
        result = await conn.fetchval("SELECT 1")
        query_time = time.time() - start_time
        logger.info(f"âœ… ç®€å•æŸ¥è¯¢: {query_time*1000:.2f}ms")
        
        # æµ‹è¯•è®¡æ•°æŸ¥è¯¢
        start_time = time.time()
        count = await conn.fetchval("SELECT COUNT(*) FROM task_queue")
        count_time = time.time() - start_time
        logger.info(f"âœ… è®¡æ•°æŸ¥è¯¢: {count_time*1000:.2f}ms (ç»“æœ: {count})")
        
        # æµ‹è¯•å‡½æ•°è°ƒç”¨
        start_time = time.time()
        task_id = await conn.fetchval("""
            SELECT enqueue_task(
                'ocr_extract'::task_type,
                '{"test": "quick_test"}'::jsonb,
                NULL,
                5,
                0,
                3,
                'quick-test'
            )
        """)
        function_time = time.time() - start_time
        logger.info(f"âœ… å‡½æ•°è°ƒç”¨: {function_time*1000:.2f}ms (ä»»åŠ¡ID: {task_id})")
        
        await conn.close()
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_connection_pool():
    """æµ‹è¯•è¿æ¥æ± æ€§èƒ½"""
    logger.info("ğŸŠ æµ‹è¯•è¿æ¥æ± æ€§èƒ½")
    
    db_url = settings.database_url.replace('postgresql+asyncpg://', 'postgresql://').replace('postgresql+psycopg://', 'postgresql://')
    
    try:
        # åˆ›å»ºè¿æ¥æ± 
        start_time = time.time()
        pool = await asyncpg.create_pool(
            db_url,
            min_size=2,
            max_size=5,
            command_timeout=30
        )
        pool_time = time.time() - start_time
        logger.info(f"âœ… è¿æ¥æ± åˆ›å»º: {pool_time*1000:.2f}ms")
        
        # æµ‹è¯•ä»æ± ä¸­è·å–è¿æ¥
        start_time = time.time()
        async with pool.acquire() as conn:
            acquire_time = time.time() - start_time
            logger.info(f"âœ… è·å–è¿æ¥: {acquire_time*1000:.2f}ms")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            start_time = time.time()
            result = await conn.fetchval("SELECT COUNT(*) FROM task_queue WHERE status = 'pending'")
            query_time = time.time() - start_time
            logger.info(f"âœ… æ± åŒ–æŸ¥è¯¢: {query_time*1000:.2f}ms (ç»“æœ: {result})")
        
        # æµ‹è¯•å¤šæ¬¡æ“ä½œ
        logger.info("ğŸ”„ æµ‹è¯•è¿ç»­æ“ä½œ...")
        
        total_start = time.time()
        for i in range(5):
            start_time = time.time()
            async with pool.acquire() as conn:
                result = await conn.fetchval("SELECT $1", i)
            op_time = time.time() - start_time
            logger.info(f"   æ“ä½œ {i+1}: {op_time*1000:.2f}ms")
        
        total_time = time.time() - total_start
        logger.info(f"âœ… 5æ¬¡æ“ä½œæ€»æ—¶é—´: {total_time*1000:.2f}ms")
        
        await pool.close()
        return True
        
    except Exception as e:
        logger.error(f"âŒ è¿æ¥æ± æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_task_queue_operations():
    """æµ‹è¯•ä»»åŠ¡é˜Ÿåˆ—æ“ä½œæ€§èƒ½"""
    logger.info("ğŸ“‹ æµ‹è¯•ä»»åŠ¡é˜Ÿåˆ—æ“ä½œæ€§èƒ½")
    
    from app.services.postgresql_task_processor import task_queue
    
    try:
        # æµ‹è¯•å…¥é˜Ÿæ“ä½œ
        logger.info("ğŸ”„ æµ‹è¯•ä»»åŠ¡å…¥é˜Ÿ...")
        
        start_time = time.time()
        task_id = await task_queue.enqueue(
            task_type="ocr_extract",
            payload={"test": "performance_test"},
            priority=1,
            correlation_id="perf-test"
        )
        enqueue_time = time.time() - start_time
        logger.info(f"âœ… ä»»åŠ¡å…¥é˜Ÿ: {enqueue_time*1000:.2f}ms (ID: {task_id})")
        
        # æµ‹è¯•çŠ¶æ€æŸ¥è¯¢
        logger.info("ğŸ” æµ‹è¯•çŠ¶æ€æŸ¥è¯¢...")
        
        start_time = time.time()
        status = await task_queue.get_task_status(task_id)
        query_time = time.time() - start_time
        logger.info(f"âœ… çŠ¶æ€æŸ¥è¯¢: {query_time*1000:.2f}ms")
        logger.info(f"   ä»»åŠ¡çŠ¶æ€: {status['status'] if status else 'None'}")
        
        # æµ‹è¯•ç»Ÿè®¡æŸ¥è¯¢
        logger.info("ğŸ“Š æµ‹è¯•ç»Ÿè®¡æŸ¥è¯¢...")
        
        start_time = time.time()
        stats = await task_queue.get_task_stats(hours_back=1)
        stats_time = time.time() - start_time
        logger.info(f"âœ… ç»Ÿè®¡æŸ¥è¯¢: {stats_time*1000:.2f}ms (ç»“æœ: {len(stats)} æ¡)")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡é˜Ÿåˆ—æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_network_latency():
    """æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ"""
    logger.info("ğŸŒ æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ")
    
    import asyncpg
    
    db_url = settings.database_url.replace('postgresql+asyncpg://', 'postgresql://').replace('postgresql+psycopg://', 'postgresql://')
    
    try:
        latencies = []
        
        for i in range(3):
            start_time = time.time()
            conn = await asyncpg.connect(db_url, statement_cache_size=0)
            await conn.fetchval("SELECT 1")
            await conn.close()
            end_time = time.time()
            
            latency = end_time - start_time
            latencies.append(latency)
            logger.info(f"   æµ‹è¯• {i+1}: {latency*1000:.2f}ms")
        
        avg_latency = sum(latencies) / len(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        
        logger.info(f"âœ… å¹³å‡å»¶è¿Ÿ: {avg_latency*1000:.2f}ms")
        logger.info(f"   æœ€å°å»¶è¿Ÿ: {min_latency*1000:.2f}ms")
        logger.info(f"   æœ€å¤§å»¶è¿Ÿ: {max_latency*1000:.2f}ms")
        
        # åˆ¤æ–­ç½‘ç»œçŠ¶å†µ
        if avg_latency < 0.1:
            logger.info("ğŸš€ ç½‘ç»œçŠ¶å†µ: ä¼˜ç§€")
        elif avg_latency < 0.5:
            logger.info("âœ… ç½‘ç»œçŠ¶å†µ: è‰¯å¥½")
        elif avg_latency < 1.0:
            logger.info("âš ï¸ ç½‘ç»œçŠ¶å†µ: ä¸€èˆ¬")
        else:
            logger.info("âŒ ç½‘ç»œçŠ¶å†µ: éœ€è¦ä¼˜åŒ–")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç½‘ç»œå»¶è¿Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False


async def diagnose_performance_issues():
    """è¯Šæ–­æ€§èƒ½é—®é¢˜"""
    logger.info("ğŸ” è¯Šæ–­æ€§èƒ½é—®é¢˜")
    
    # æ£€æŸ¥é…ç½®
    logger.info("âš™ï¸ æ£€æŸ¥é…ç½®...")
    logger.info(f"   æ•°æ®åº“URLåŒ…å«pooler: {'pooler' in settings.database_url}")
    logger.info(f"   æ•°æ®åº“URLåŒ…å«supabase: {'supabase' in settings.database_url}")
    logger.info(f"   è°ƒè¯•æ¨¡å¼: {settings.debug}")
    
    # æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
    db_url = settings.database_url.replace('postgresql+asyncpg://', 'postgresql://').replace('postgresql+psycopg://', 'postgresql://')
    
    try:
        conn = await asyncpg.connect(db_url, statement_cache_size=0)
        
        # æ£€æŸ¥æ´»è·ƒè¿æ¥æ•°
        active_connections = await conn.fetchval("""
            SELECT count(*) 
            FROM pg_stat_activity 
            WHERE state = 'active'
        """)
        logger.info(f"   æ´»è·ƒè¿æ¥æ•°: {active_connections}")
        
        # æ£€æŸ¥æ•°æ®åº“ç‰ˆæœ¬
        version = await conn.fetchval("SELECT version()")
        logger.info(f"   æ•°æ®åº“ç‰ˆæœ¬: {version[:50]}...")
        
        # æ£€æŸ¥è¡¨å¤§å°
        table_size = await conn.fetchval("""
            SELECT pg_size_pretty(pg_total_relation_size('task_queue'))
        """)
        logger.info(f"   task_queueè¡¨å¤§å°: {table_size}")
        
        # æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
        index_usage = await conn.fetch("""
            SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch
            FROM pg_stat_user_indexes 
            WHERE tablename = 'task_queue'
        """)
        
        if index_usage:
            logger.info("   ç´¢å¼•ä½¿ç”¨æƒ…å†µ:")
            for idx in index_usage:
                logger.info(f"     {idx['indexname']}: è¯»å– {idx['idx_tup_read']}, è·å– {idx['idx_tup_fetch']}")
        else:
            logger.warning("   âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡")
        
        await conn.close()
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ PostgreSQLä»»åŠ¡é˜Ÿåˆ—å¿«é€Ÿæ€§èƒ½è¯Šæ–­")
    logger.info("=" * 60)
    
    tests = [
        ("ç½‘ç»œå»¶è¿Ÿæµ‹è¯•", test_network_latency),
        ("åŸå§‹æ•°æ®åº“è¿æ¥", test_raw_database_connection),
        ("è¿æ¥æ± æ€§èƒ½", test_connection_pool),
        ("ä»»åŠ¡é˜Ÿåˆ—æ“ä½œ", test_task_queue_operations),
    ]
    
    results = {}
    
    # å…ˆè¯Šæ–­
    await diagnose_performance_issues()
    
    logger.info("\n" + "=" * 60)
    logger.info("å¼€å§‹æ€§èƒ½æµ‹è¯•")
    logger.info("=" * 60)
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*20} {test_name} {'='*20}")
        
        try:
            start_time = time.time()
            result = await test_func()
            end_time = time.time()
            
            test_duration = end_time - start_time
            results[test_name] = {
                "success": result,
                "duration": test_duration
            }
            
            status = "âœ… æˆåŠŸ" if result else "âŒ å¤±è´¥"
            logger.info(f"{status} - è€—æ—¶: {test_duration:.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            results[test_name] = {"success": False, "error": str(e)}
    
    # æ€»ç»“
    logger.info("\n" + "=" * 60)
    logger.info("æ€§èƒ½è¯Šæ–­æ€»ç»“")
    logger.info("=" * 60)
    
    successful_tests = sum(1 for r in results.values() if r.get("success", False))
    total_tests = len(results)
    
    logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ: {successful_tests}/{total_tests} æˆåŠŸ")
    
    for test_name, result in results.items():
        if result.get("success"):
            logger.info(f"âœ… {test_name}: {result['duration']:.2f}s")
        else:
            logger.info(f"âŒ {test_name}: å¤±è´¥")
    
    # æ€§èƒ½å»ºè®®
    logger.info("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    
    if successful_tests == 0:
        logger.info("ğŸ”§ ä¸¥é‡æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥:")
        logger.info("   - ç½‘ç»œè¿æ¥åˆ°Supabase")
        logger.info("   - æ•°æ®åº“é…ç½®å’Œæƒé™")
        logger.info("   - é˜²ç«å¢™å’Œä»£ç†è®¾ç½®")
    elif successful_tests < total_tests:
        logger.info("ğŸ”§ éƒ¨åˆ†åŠŸèƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®:")
        logger.info("   - å®ç°è¿æ¥æ± ä»¥å¤ç”¨è¿æ¥")
        logger.info("   - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢")
        logger.info("   - è€ƒè™‘æ·»åŠ ç¼“å­˜å±‚")
    else:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        logger.info("ğŸ”§ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
        logger.info("   - ç›‘æ§ç”Ÿäº§ç¯å¢ƒæ€§èƒ½")
        logger.info("   - è€ƒè™‘æ‰¹é‡æ“ä½œä¼˜åŒ–")
    
    return 0 if successful_tests > 0 else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)